{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi,cos,radians\n",
    "import pyproj\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.geometry import Point\n",
    "import shapely.geometry as shpg\n",
    "import salem\n",
    "import datetime as dt\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\LeBris_Paul\\\\ElevationChange_GlaciersSupp1km2_LessThan20%DataVoid.shp' # LeBris/Paul shape\n",
    "foga_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\DOI-WGMS-FoG-2015-11\\\\WGMS-FoG-2015-11-A-GENERAL-INFORMATION.csv'\n",
    "rgi_ak_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\01_rgi50_Alaska\\\\01_rgi50_Alaska_selecxt_LeBris_2km.shp'\n",
    "wgms_subregions_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\Glacier_Subregions_jhuber\\\\Glacier_Subregions_jhuber.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp = gpd.read_file(lp_p)\n",
    "rgi_ak = gpd.read_file(rgi_ak_p)\n",
    "lp['ID'] = lp['ID'].astype(int)\n",
    "foga = pd.read_csv(foga_p, encoding='iso-8859-1')\n",
    "wgms_subregions = gpd.read_file(wgms_subregions_p)\n",
    "#wgms_subregions = wgms_subregions[wgms_subregions.WGMS_Code.str.contains('ALA')] # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an empty FoG-D DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fogd_templ = pd.DataFrame([], [['POLITICAL_UNIT', 'GLACIER_NAME', 'WGMS_ID', 'YEAR', 'LOWER_BOUND', 'UPPER_BOUND', \n",
    "                                'AREA_SURVEY_YEAR', 'AREA_CHANGE', 'AREA_CHANGE_UNCERTAINTY', 'THICKNESS_CHANGE', \n",
    "                                'THICKNESS_CHANGE_UNCERTAINTY', 'VOLUME_CHANGE', 'VOLUME_CHANGE_UNCERTAINTY', 'SURVEY_DATE',\n",
    "                                'SURVEY_DATE_PLATFORM_METHOD', 'REFERENCE_DATE', 'REFERENCE_DATE_PLATFORM_METHOD',\n",
    "                                'INVESTIGATOR', 'SPONSORING_AGENCY', 'REFERENCE', 'REMARKS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine uncertainty for given FoG coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coord_unc(x,y):\n",
    "    \"\"\" Determine the uncertainity in meters for a given FoG coordinate at its lat/lon (assuming earth was a sphere)\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x: longitude in degrees\n",
    "    y: latitude in degrees\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    dx, dy: uncertainty in meters\n",
    "    \"\"\"\n",
    "    # assert given numbers are floats\n",
    "    # and determine number of decimals given\n",
    "    if isinstance(x, int):\n",
    "        xd = 0\n",
    "    elif isinstance(x, float):\n",
    "        xd = str(x)[::-1].find('.')\n",
    "    else:\n",
    "        raise TypeError('Given x must be float or integer.)')\n",
    "        \n",
    "    if isinstance(y, int):\n",
    "        yd = 0\n",
    "    elif isinstance(y, float):\n",
    "        yd = str(y)[::-1].find('.')\n",
    "    else:\n",
    "        raise TypeError('Given y must be float or integer.)')\n",
    "    \n",
    "    # maximum error in degrees\n",
    "    xe = 10**(-xd)\n",
    "    ye = 10**(-yd)\n",
    "    \n",
    "    # maximum error in meters\n",
    "    dy = ye * ((2.* pi * 6378000.) / 360.)\n",
    "    dx = xe * (2. * pi * cos(radians(y)) * 6378000.) / 360.\n",
    "    \n",
    "    print(dx, dy)\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_coords(x,y,in_proj=None,out_proj=None):\n",
    "    \n",
    "    assert in_proj is not None,'Input projection may not be None.'\n",
    "    assert out_proj is not None,'Output projection may not be None.'\n",
    "    from_proj = pyproj.Proj(init='epsg:'+str(in_proj))\n",
    "    to_proj = pyproj.Proj(init='epsg:'+str(out_proj))\n",
    "    \n",
    "    #x1, y1 = from_proj(x,y)\n",
    "    return pyproj.transform(from_proj, to_proj, x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a buffer around all FoG points in the area, determined by the uncertainty of the coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the two Projs we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utm7n = pyproj.Proj(init='epsg:32605') # WGS84 UTM 5N -> LeBris/Paul\n",
    "latlon = pyproj.Proj(init='epsg:4326') # WGS84 lat/lon -> FoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the FoG points with the extent rectangle by LeBris/Paul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp_extent = lp.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert UTM5N to WGS84 lat/lon points\n",
    "x_lp = [lp_extent[0], lp_extent[2]]\n",
    "y_lp = [lp_extent[1], lp_extent[3]]\n",
    "x1,y1 = utm7n(x_lp, y_lp)\n",
    "x_lpll, y_lpll = pyproj.transform(utm7n,latlon,x_lp,y_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foga = foga[(foga.LONGITUDE >= x_lpll[0]) & (foga.LONGITUDE <= x_lpll[1]) & (foga.LATITUDE >= y_lpll[0]) & (foga.LATITUDE <= y_lpll[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a buffer around them, determined by the uncertainty of the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert WGS84 lat/lon points to UTM5N\n",
    "xs = foga.LONGITUDE.values\n",
    "ys = foga.LATITUDE.values\n",
    "x1,y1 = latlon(xs, ys)\n",
    "xm, ym = pyproj.transform(latlon,utm7n,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffers = [Point(x,y).buffer(coord_unc(transform_coords(x,y,32605,4326)[0],transform_coords(x,y,32605,4326)[1])[1]) for x,y in zip(xm,ym)]\n",
    "#buffers = [Point(x,y).buffer(np.nanmax(coord_unc(x,y))) for x,y in zip(xs,ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the buffer one geometry\n",
    "buffer_union = cascaded_union(buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a series of true/false of those that do not intersect with the buffer should be tried to be directly imported into FoG (there WILL be mistakes) !!!\n",
    "disjoint = lp.disjoint(buffer_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disjoint_ix = disjoint[disjoint == True].index\n",
    "intersect_ix = disjoint[disjoint == False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the real geometries\n",
    "fast_to_FoG = lp[lp.index.isin(disjoint_ix)]\n",
    "check_closely = lp[lp.index.isin(intersect_ix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(fast_to_FoG)+len(check_closely)==len(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fast_to_FoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_a = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='A - GENERAL INFORMATION')\n",
    "template_b = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='B - STATE')\n",
    "template_d = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='D - CHANGE')\n",
    "\n",
    "template_a['GEO-REGION_CODE'] = ''\n",
    "template_a['GEO-SUBREGION_CODE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assign_dict = {(3373, 'AIALIK'): np.nan,    # no equivalent \n",
    "               (3543, 'ALLEN'): np.nan,     # no equivalent\n",
    "              (92, 'APPLEGATE'): 119,\n",
    "              (102, 'BAKER'): 280,\n",
    "              (105, 'BALTIMORE'): 214,\n",
    "              (165, 'BARNARD'): 184,\n",
    "              (168, 'BARRY'): 971,\n",
    "              (1390, 'BARTLETT'): np.nan,  # no equ\n",
    "              (3372, 'BEAR'): np.nan,      # no equ\n",
    "              (3376, 'BEAR LAKE'): np.nan, # no eq \n",
    "              (97, 'BELOIT'): 302,\n",
    "              (3377, 'BENCH'): np.nan,\n",
    "              (98,'BLACKSTONE'): 980,\n",
    "              (157,'BRILLIANT'): np.nan,\n",
    "              (162,'BRYN MAWR'): 977,\n",
    "              (169,'CASCADE'): 973,\n",
    "              (100,'CATARACT'): 33,\n",
    "              (180,'CHENEGA'): np.nan,\n",
    "              (3379,'CHERNOF'): np.nan,\n",
    "              (152,'CHILDS'): np.nan,\n",
    "              (176,'CLAREMONT NORTH'): 311,\n",
    "              (177,'CLAREMONT WEST'): 312,\n",
    "              (3544,'COLONY'): 203,\n",
    "              (156,'COLUMBIA (627)'): 976,\n",
    "              (178,'CONTACT'): 325,   # is okay as only front variation present in FoG\n",
    "              (167,'COXE'): 978,\n",
    "              (3381,'DESERTED'): np.nan,\n",
    "              (101,'DETACHED'): np.nan,\n",
    "              (3382,'DINGLESTADT'): 364,\n",
    "              (3383,'DOUBLE'): 774,   # ATTENTION: Name is different: DOUBLE in FoG, Big RIver Glacier in LP and Big River Lobe Double Glac in GLIMS\n",
    "              (85,'EKLUTNA'): np.nan,\n",
    "              (3790,'EXCELSIOR'): 981,\n",
    "              (86,'EXIT'): np.nan,\n",
    "              (91,'FALLING'): np.nan,\n",
    "              (3386,'FORK TLIKAKILA'): 777,\n",
    "              (3791,'GREWINGK'): np.nan,\n",
    "              (172,'HARRIMAN'): 975,\n",
    "              (160,'HARVARD'): np.nan,\n",
    "              (3390,'HOLGATE'): 982,\n",
    "              (166,'HOLYOKE'): 185,\n",
    "              (3391,'KACHEMAK'): np.nan,\n",
    "              (3310,'KNIK'): 204,\n",
    "              (4333,'KNIK NORTH'): np.nan,  # part of KNIK (only one polygon in LP)\n",
    "              (4332,'KNIK SOUTH'): np.nan,  # part of KNIK (only one polygon in LP)\n",
    "              (95,'LAWRENCE'): 298,\n",
    "              (173,'LEARNARD'): 100,\n",
    "              (3394,'LITTLE DINGLESTADT'): 360,\n",
    "              (3545,'MARCUS BAKER'): 177,\n",
    "              (96,'MARQUETTE'): 301,\n",
    "              (3546,'MATANUSKA E'): np.nan,  # one polygon together with MATANUSKA W in LP\n",
    "              (3547,'MATANUSKA W'): np.nan,  # one polygon together with MATANUSKA E in LP\n",
    "              (3396,'MC CARTY'): np.nan,\n",
    "              (158,'MEARES'): np.nan,\n",
    "              (3548,'NELCHINA'): np.nan,\n",
    "              (179,'NELLIE JUAN'): np.nan,\n",
    "              (3414,'NORTH FORK TLIKAKILA'): 769,\n",
    "              (3399,'NORTHEASTERN'): 366,    # not absolutely sure if this is the one\n",
    "              (3793,'NORTHWESTERN'): np.nan,\n",
    "              (3794,'NUKA'): np.nan,         # VERY DIFFICULT CASE\n",
    "              (103,'PENNIMAN EAST'): np.nan, # not really clear where the transition is\n",
    "              (104,'PENNIMAN WEST'): 279,    # probably true but not 100% clear \n",
    "              (174,'PORTAGE'): 292,\n",
    "              (3335,'RED'): np.nan,           \n",
    "              (99,'ROARING'): np.nan,\n",
    "              (108,'SADDLEBAG'): np.nan,\n",
    "              (4334,'SCOTT'): np.nan,\n",
    "              (170,'SERPENTINE'): 979,\n",
    "              (3413,'SHAMROCK'): 781,\n",
    "              (151,'SHERIDAN'): np.nan,\n",
    "              (107,'SHERMAN'): np.nan,\n",
    "              (155,'SHOUP'): 970,\n",
    "              (3401,'SKILAK'): np.nan,\n",
    "              (161,'SMITH'): np.nan,\n",
    "              (3549,'SOUTH FORK TSINA'): 179,  # very strange naming, but this is it\n",
    "              (1391,'SPENCER'): np.nan,        # FoG coordinate was wrong\n",
    "              (171,'SURPRISE'): 974,\n",
    "              (3403,'TANAINA'): 792,\n",
    "              (93,'TAYLOR US'): 310,\n",
    "              (3405,'TAZLINA'): np.nan,\n",
    "              (175,'TEBENKOF'): np.nan,\n",
    "              (3550,'TONSINA'): 195,\n",
    "              (1389,'TRAIL'): 307,\n",
    "              (3551,'TSINA'): 178,\n",
    "              (3407,'TURQUOISE'): 787,\n",
    "              (3408,'TUSTUMENA'): np.nan,\n",
    "              (3409,'TUXEDNI'): 928, \n",
    "              (1387,'UNNAMED US0623'): np.nan,    # seems to be part of ID 976 in LP as B table says only 4km long\n",
    "              (106,'UNNAMED US624'): np.nan,      # unclear which is meant; probably no equivalent\n",
    "              (154,'VALDEZ'): 211,\n",
    "              (163,'VASSAR'): 215,\n",
    "              (164,'WELLESLEY'): 972,\n",
    "              (94,'WOLVERINE'): np.nan,\n",
    "              (3580,'WOODWORTH'): np.nan,\n",
    "              (153,'WORTHINGTON'): 181,\n",
    "              (3412,'WORTHMANNS'): np.nan,\n",
    "              (159,'YALE'): np.nan,\n",
    "              (3797,'YALIK'):409}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "political_unit = 'US'\n",
    "geogr_loc = 'Alaska'\n",
    "year = 2006\n",
    "survey_date = 20069999\n",
    "ref_date = 19509999\n",
    "geo_code = 'ALA'\n",
    "surv_plat_meth = 'sP'\n",
    "ref_plat_meth = 'aM'\n",
    "remarks_a = ''\n",
    "remarks_b = 'Outlines derived by manual/automated digitizing based on large-scale 7.5 min topographic quadrangle maps - some corrected manually. Geometrical attributes are taken from RGI5.0 and might be from up to four years around the survey date.'\n",
    "remarks_d = '1950s: Outlines and elevation derived from topographic maps, 2006: elevation derived from SPOT5 HRS (SPIRIT). '\n",
    "investigator = 'Raymond LE BRIS and Frank PAUL'\n",
    "spons_agenc = 'Dept. of Geography, University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Switzerland'\n",
    "ref = 'Le Bris, R. and Paul, F. (2015); Annals of Glaciology, 56(70), pp.184-192.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp = lp.to_crs({'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp.to_file('c:\\\\users\\jlandman\\\\desktop\\\\LPGeometries.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgi_ak.to_file('c:\\\\users\\jlandman\\\\desktop\\\\RGIGeometries.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ids = range(5871, 6871, 1)\n",
    "\n",
    "# create lists with areas in order to check whether LeBris/RGI polygons match\n",
    "area_list_LP = []\n",
    "area_list_RGI = []\n",
    "\n",
    "no_probs_LP = []\n",
    "no_probs_RGI = []\n",
    "\n",
    "ct=0\n",
    "fpct=0\n",
    "\n",
    "\n",
    "#lp_copy = lp.copy()\n",
    "for ind, row in lp.iterrows():\n",
    "    ct+=1\n",
    "    #important to reset!\n",
    "    match_ix = None\n",
    "    \n",
    "    # this is to check agreement with RGI5.0 on the geometric way....catastrophe (161 matches when maximum intersection plus 10% area tolerance is chosen, 11 matches when RGI point loaction is checked in polygons plus 10% area tolerance)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #match_ix = rgi_ak.intersects(row.geometry).values.tolist().index(True)\n",
    "        match_ix = np.where(rgi_ak.intersects(row.geometry).values == True)[0].tolist()\n",
    "    except ValueError:  # in case there is no intersection, there won't be any TRUE in the list\n",
    "        pass\n",
    "    \n",
    "    if len(match_ix)==1:\n",
    "        rgi_match = rgi_ak[rgi_ak.index.isin(match_ix)]\n",
    "    elif len(match_ix) > 1: # if there is more than one intersection, take the one with the biggest area overlap\n",
    "        rgi_match = rgi_ak[rgi_ak.index.isin(match_ix)]\n",
    "        rgi_match['union_area'] = lp[lp.index==ind].geometry.intersection(rgi_match.geometry).area\n",
    "        rgi_match = rgi_match[rgi_match.index ==rgi_match['union_area'].argmax()]\n",
    "    else:\n",
    "        rgi_match = pd.DataFrame([])\n",
    "    \n",
    "    if rgi_match.empty:\n",
    "        rgi_match = pd.DataFrame(data=np.nan * np.ones(shape=(1,19)), columns=rgi_ak.columns.values)\n",
    "    \n",
    "    #print('RGI Area: %s, LP Area: %s' % (rgi_match.Area, row.AREA))\n",
    "    if not (rgi_match.Area.iloc[0]*0.9 < row.AREA < rgi_match.Area.iloc[0]*1.1):\n",
    "        print('Flaechenproblem:', rgi_match.RGIId.iloc[0])\n",
    "        fpct+=1\n",
    "    else:\n",
    "        no_probs_LP.append(row.AREA)\n",
    "        no_probs_RGI.append(rgi_match.Area.iloc[0])\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    remarks_a_thistime = ''\n",
    "    remarks_d_thistime = ''\n",
    "    \n",
    "    if row.ID not in assign_dict.values():\n",
    "        wgms_id = new_ids[0]\n",
    "        new_ids = new_ids[1:]\n",
    "        \n",
    "        template_a.loc[ind, 'POLITCAL_UNIT'] = political_unit\n",
    "        template_a.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        if row.glacier_na != None:\n",
    "            template_a.loc[ind, 'GLACIER_NAME'] = row.glacier_na.upper()\n",
    "        else:\n",
    "            template_a.loc[ind, 'GLACIER_NAME'] = 'UNNAMED %s' % row.ID\n",
    "            remarks_a_thistime = 'Glacier number in GLACIER_NAME from Le Bris, R. and Paul, F. (2015): Annals of Glaciology 56(70), pp.184-192.'\n",
    "        template_a.loc[ind, 'GEOGRAPHICAL_LOCATION_GENERAL']  = geogr_loc\n",
    "        \n",
    "        #print(rgi_match.empty)\n",
    "        #if pd.isnull(rgi_match.CenLat.iloc[0]) or pd.isnull(rgi_match.CenLon.iloc[0]):\n",
    "        repres_point_ll = row.geometry.representative_point()\n",
    "            #repres_point_ll = transform_coords(repres_point.x, repres_point.y,32605,4326) \n",
    "        #else:\n",
    "        #    repres_point_ll = Point((rgi_match.CenLon.iloc[0], rgi_match.CenLat.iloc[0]))\n",
    "        \n",
    "        template_a.loc[ind, 'LATITUDE'] = round(repres_point_ll.y, 6)\n",
    "        template_a.loc[ind, 'LONGITUDE'] = round(repres_point_ll.x, 6)\n",
    "        template_a.loc[ind, 'REMARKS'] = remarks_a + remarks_a_thistime\n",
    "        if row.Type == 1 or row.Type == 2: # means: lake-/tidewater-terminating\n",
    "            template_a.loc[ind, 'FRONTAL_CHARACTERISTICS'] = 4   # means: calving\n",
    "        \n",
    "        template_a.loc[ind, 'GEO-REGION_CODE'] = geo_code\n",
    "        for k,r in wgms_subregions.iterrows():\n",
    "            if r.geometry.contains(Point(round(repres_point_ll.x, 6), round(repres_point_ll.y, 6))):\n",
    "                geo_sub_code = r.WGMS_Code\n",
    "                \n",
    "        template_a.loc[ind, 'GEO-SUBREGION_CODE'] = geo_sub_code\n",
    "        \"\"\"template_b.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        template_b.loc[ind, 'GLACIER_NAME'] = row.glacier_na.upper()\n",
    "        template_b.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_b.loc[ind, 'YEAR'] = year\n",
    "        template_b.loc[ind, 'MAXIMUM_ELEVATION_OF_GLACIER'] = rgi_match.Zmax.iloc[0]\n",
    "        template_b.loc[ind, 'MEDIAN_ELEVATION_OF_GLACIER'] = rgi_match.Zmed.iloc[0]\n",
    "        template_b.loc[ind, 'MINIMUM_ELEVATION_OF_GLACIER'] = rgi_match.Zmin.iloc[0]\n",
    "        template_b.loc[ind, 'LENGTH'] = rgi_match.Lmax.iloc[0]\n",
    "        template_b.loc[ind, 'AREA'] = rgi_match.Area.iloc[0]\n",
    "        template_b.loc[ind, 'SURVEY_DATE'] = rgi_match.BgnDate.iloc[0]\n",
    "        template_b.loc[ind, 'SURVEY_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_b.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_b.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_b.loc[ind, 'REFERENCE'] = ref\n",
    "        template_b.loc[ind, 'REMARKS'] = remarks_b + ''\n",
    "\"\"\"\n",
    "        template_d.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        if row.glacier_na != None:\n",
    "            template_d.loc[ind, 'GLACIER_NAME'] = row.glacier_na.upper()\n",
    "        else:\n",
    "            template_d.loc[ind, 'GLACIER_NAME'] = 'UNNAMED %s' % row.ID\n",
    "            remarks_d_thistime = 'Glacier number in GLACIER_NAME from Le Bris, R. and Paul, F. (2015): Annals of Glaciology 56(70), pp.184-192. '\n",
    "        template_d.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_d.loc[ind, 'YEAR'] = year\n",
    "        template_d.loc[ind, 'LOWER_BOUND'] = 9999.\n",
    "        template_d.loc[ind, 'UPPER_BOUND'] = 9999.\n",
    "        template_d.loc[ind, 'AREA_SURVEY_YEAR'] = row.AREA\n",
    "        template_d.loc[ind, 'THICKNESS_CHANGE'] = round(row.dh_mean * 1000.)  # unit: mm\n",
    "        template_d.loc[ind, 'VOLUME_CHANGE'] = round(row.dh_mean * row.AREA * 1000.)  # unit: m*km2*1000 = 1000m3\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'REFERENCE_DATE_PLATFORM_METHOD'] = ref_plat_meth\n",
    "        template_d.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_d.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_d.loc[ind, 'REFERENCE'] = ref\n",
    "        template_d.loc[ind, 'REMARKS'] = remarks_d + remarks_d_thistime + 'Data void on this glacier is %s%%.' % \"{0:.2f}\".format(row.data_void)\n",
    "    \n",
    "    else:\n",
    "        wgms_id = list(assign_dict.keys())[list(assign_dict.values()).index(row.ID)][0]\n",
    "        gname = list(assign_dict.keys())[list(assign_dict.values()).index(row.ID)][1]\n",
    "        \n",
    "        \"\"\"\n",
    "        template_b.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        template_b.loc[ind, 'GLACIER_NAME'] = gname.upper()\n",
    "        template_b.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_b.loc[ind, 'YEAR'] = year\n",
    "        template_b.loc[ind, 'MAXIMUM_ELEVATION_OF_GLACIER'] = rgi_match.Zmax.iloc[0]\n",
    "        template_b.loc[ind, 'MEDIAN_ELEVATION_OF_GLACIER'] = rgi_match.Zmed.iloc[0]\n",
    "        template_b.loc[ind, 'MINIMUM_ELEVATION_OF_GLACIER'] = rgi_match.Zmin.iloc[0]\n",
    "        template_b.loc[ind, 'LENGTH'] = rgi_match.Lmax.iloc[0]\n",
    "        template_b.loc[ind, 'AREA'] = rgi_match.Area.iloc[0]\n",
    "        template_b.loc[ind, 'SURVEY_DATE'] = rgi_match.BgnDate.iloc[0]\n",
    "        template_b.loc[ind, 'SURVEY_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_b.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_b.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_b.loc[ind, 'REFERENCE'] = ref\n",
    "        template_b.loc[ind, 'REMARKS'] = remarks_b + ''\n",
    "        \"\"\"\n",
    "\n",
    "        template_d.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        template_d.loc[ind, 'GLACIER_NAME'] = gname.upper()\n",
    "        template_d.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_d.loc[ind, 'YEAR'] = year\n",
    "        template_d.loc[ind, 'LOWER_BOUND'] = 9999.\n",
    "        template_d.loc[ind, 'UPPER_BOUND'] = 9999.\n",
    "        template_d.loc[ind, 'AREA_SURVEY_YEAR'] = row.AREA\n",
    "        template_d.loc[ind, 'THICKNESS_CHANGE'] = round(row.dh_mean * 1000.)  # unit: mm\n",
    "        template_d.loc[ind, 'VOLUME_CHANGE'] = round(row.dh_mean * row.AREA * 1000.)  # unit: m*km2*1000 = 1000m3\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'REFERENCE_DATE_PLATFORM_METHOD'] = ref_plat_meth\n",
    "        template_d.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_d.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_d.loc[ind, 'REFERENCE'] = ref\n",
    "        template_d.loc[ind, 'REMARKS'] = remarks_d + remarks_d_thistime + 'Data void on this glacier is %s%%.' % \"{0:.2f}\".format(row.data_void)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "area_list_LP_na = area_list_LP\n",
    "area_list_RGI_na = area_list_RGI\n",
    "for n in range(len(area_list_LP)):\n",
    "    if not (area_list_RGI[n]*0.9 < area_list_LP[n] < area_list_RGI[n]*1.1):\n",
    "        area_list_LP_na[n] = np.nan\n",
    "        area_list_RGI_na[n] = np.nan\n",
    "len(area_list_RGI_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "area_list_RGI_na.count(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.scatter(area_list_LP_na, area_list_RGI_na)\n",
    "plt.xlim((0,12))\n",
    "plt.ylim((0,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.scatter(no_probs_LP, no_probs_RGI)\n",
    "plt.xlim((0,100))\n",
    "plt.ylim((0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_a.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_A.xls', index=False)\n",
    "template_b.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_B.xls', index=False)\n",
    "template_d.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_D.xls', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign GLIMS IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glims = salem.utils.read_shapefile('C:\\\\Users\\\\jlandman\\\\Desktop\\\\glims_db_20160429\\\\glims_polygons_alaska.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glims_id_success = {}\n",
    "glims_id_fail = {}\n",
    "\n",
    "invest_date = dt.datetime.strptime(str(survey_date), '%Y%m%d') if str(survey_date)[4:]!='9999' else dt.datetime.strptime(str(survey_date)[:4]+'0630', '%Y%m%d')\n",
    "ct=0\n",
    "for k, row in template_a.iterrows():\n",
    "    gp = shpg.Point(row.LONGITUDE, row.LATITUDE)\n",
    "    rectangle = shpg.Polygon([(row.LONGITUDE-0.00001, row.LATITUDE-0.00001), (row.LONGITUDE+0.00001, row.LATITUDE-0.00001), (row.LONGITUDE+0.00001, row.LATITUDE+0.00001), (row.LONGITUDE-0.00001, row.LATITUDE+0.00001)])\n",
    "    subset = glims[glims.intersects(rectangle)]\n",
    "    if len(subset) == 1:\n",
    "        if subset.geometry.iloc[0].contains(gp):\n",
    "            glims_id_success[row.WGMS_ID] = subset['glac_id'].iloc[0]\n",
    "        else:\n",
    "            glims_id_fail[row.WGMS_ID] = np.nan\n",
    "    elif len(subset) > 1:\n",
    "        glims_inv = [dt.datetime.strptime(d, '%Y-%m-%dT%H:%M:%S') for d in subset.src_date.values]    # GLIMS investigation date\n",
    "        date_diffs = [abs(invest_date-i) for i in glims_inv]  # absolute date differences to get the closest investigation date\n",
    "        ix_date_close = date_diffs.index(min(date_diffs))  # find index of the closest date\n",
    "        glims_id_close = subset.iloc[ix_date_close].glac_id # get closest GLIMS ID\n",
    "        glims_id_success[row.WGMS_ID] = glims_id_close\n",
    "    else:\n",
    "        glims_id_fail[row.WGMS_ID] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glims_id_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glims_id_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyList = glims_id_success.keys()\n",
    "valueList = glims_id_success.values()\n",
    "\n",
    "rows = zip(keyList, valueList)\n",
    "\n",
    "with open('C:\\\\Users\\\\jlandman\\\\Desktop\\\\Links_GLIMS_FoG_AK.csv', 'w') as f: \n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    for row in rows:\n",
    "        writer.writerow((row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "GLIMS_pairs = list(set(valueList)-set(rgi_ak.GLIMSId.values))\n",
    "len(GLIMS_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "gl_rgi = []\n",
    "gl_lp = []\n",
    "\n",
    "for gl_id in GLIMS_pairs:\n",
    "    gl_rgi.append(rgi_ak[rgi_ak.GLIMSId == gl_id])\n",
    "    gl_lp.append(lp[lp.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lp.ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
