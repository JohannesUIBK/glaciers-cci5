{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi,cos,radians\n",
    "import pyproj\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.geometry import Point\n",
    "import shapely.geometry as shpg\n",
    "import salem\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\LeBris_Paul\\\\ElevationChange_GlaciersSupp1km2_LessThan20%DataVoid.shp' # LeBris/Paul shape\n",
    "foga_p = 'C:\\\\Users\\\\jlandman\\\\Desktop\\\\DOI-WGMS-FoG-2015-11\\\\WGMS-FoG-2015-11-A-GENERAL-INFORMATION.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp = gpd.read_file(lp_p)\n",
    "lp['ID'] = lp['ID'].astype(int)\n",
    "foga = pd.read_csv(foga_p, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an empty FoG-D DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fogd_templ = pd.DataFrame([], [['POLITICAL_UNIT', 'GLACIER_NAME', 'WGMS_ID', 'YEAR', 'LOWER_BOUND', 'UPPER_BOUND', \n",
    "                                'AREA_SURVEY_YEAR', 'AREA_CHANGE', 'AREA_CHANGE_UNCERTAINTY', 'THICKNESS_CHANGE', \n",
    "                                'THICKNESS_CHANGE_UNCERTAINTY', 'VOLUME_CHANGE', 'VOLUME_CHANGE_UNCERTAINTY', 'SURVEY_DATE',\n",
    "                                'SURVEY_DATE_PLATFORM_METHOD', 'REFERENCE_DATE', 'REFERENCE_DATE_PLATFORM_METHOD',\n",
    "                                'INVESTIGATOR', 'SPONSORING_AGENCY', 'REFERENCE', 'REMARKS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine uncertainty for given FoG coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coord_unc(x,y):\n",
    "    \"\"\" Determine the uncertainity in meters for a given FoG coordinate at its lat/lon (assuming earth was a sphere)\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x: longitude in degrees\n",
    "    y: latitude in degrees\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    dx, dy: uncertainty in meters\n",
    "    \"\"\"\n",
    "    # assert given numbers are floats\n",
    "    # and determine number of decimals given\n",
    "    if isinstance(x, int):\n",
    "        xd = 0\n",
    "    elif isinstance(x, float):\n",
    "        xd = str(x)[::-1].find('.')\n",
    "    else:\n",
    "        raise TypeError('Given x must be float or integer.)')\n",
    "        \n",
    "    if isinstance(y, int):\n",
    "        yd = 0\n",
    "    elif isinstance(y, float):\n",
    "        yd = str(y)[::-1].find('.')\n",
    "    else:\n",
    "        raise TypeError('Given y must be float or integer.)')\n",
    "    \n",
    "    # maximum error in degrees\n",
    "    xe = 10**(-xd)\n",
    "    ye = 10**(-yd)\n",
    "    \n",
    "    # maximum error in meters\n",
    "    dy = ye * ((2.* pi * 6378000.) / 360.)\n",
    "    dx = xe * (2. * pi * cos(radians(y)) * 6378000.) / 360.\n",
    "    \n",
    "    print(dx, dy)\n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_coords(x,y,in_proj=None,out_proj=None):\n",
    "    \n",
    "    assert in_proj is not None,'Input projection may not be None.'\n",
    "    assert out_proj is not None,'Output projection may not be None.'\n",
    "    from_proj = pyproj.Proj(init='epsg:'+str(in_proj))\n",
    "    to_proj = pyproj.Proj(init='epsg:'+str(out_proj))\n",
    "    \n",
    "    #x1, y1 = from_proj(x,y)\n",
    "    return pyproj.transform(from_proj, to_proj, x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a buffer around all FoG points in the area, determined by the uncertainty of the coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the two Projs we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utm7n = pyproj.Proj(init='epsg:32605') # WGS84 UTM 5N -> LeBris/Paul\n",
    "latlon = pyproj.Proj(init='epsg:4326') # WGS84 lat/lon -> FoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the FoG points with the extent rectangle by LeBris/Paul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp_extent = lp.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert UTM5N to WGS84 lat/lon points\n",
    "x_lp = [lp_extent[0], lp_extent[2]]\n",
    "y_lp = [lp_extent[1], lp_extent[3]]\n",
    "x1,y1 = utm7n(x_lp, y_lp)\n",
    "x_lpll, y_lpll = pyproj.transform(utm7n,latlon,x_lp,y_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foga = foga[(foga.LONGITUDE >= x_lpll[0]) & (foga.LONGITUDE <= x_lpll[1]) & (foga.LATITUDE >= y_lpll[0]) & (foga.LATITUDE <= y_lpll[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(foga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a buffer around them, determined by the uncertainty of the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert WGS84 lat/lon points to UTM5N\n",
    "xs = foga.LONGITUDE.values\n",
    "ys = foga.LATITUDE.values\n",
    "x1,y1 = latlon(xs, ys)\n",
    "xm, ym = pyproj.transform(latlon,utm7n,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffers = [Point(x,y).buffer(coord_unc(transform_coords(x,y,32605,4326)[0],transform_coords(x,y,32605,4326)[1])[1]) for x,y in zip(xm,ym)]\n",
    "#buffers = [Point(x,y).buffer(np.nanmax(coord_unc(x,y))) for x,y in zip(xs,ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the buffer one geometry\n",
    "buffer_union = cascaded_union(buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buffer_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a series of true/false of those that do not intersect with the buffer should be tried to be directly imported into FoG (there WILL be mistakes) !!!\n",
    "disjoint = lp.disjoint(buffer_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disjoint_ix = disjoint[disjoint == True].index\n",
    "intersect_ix = disjoint[disjoint == False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disjoint_ix\n",
    "intersect_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the real geometries\n",
    "fast_to_FoG = lp[lp.index.isin(disjoint_ix)]\n",
    "check_closely = lp[lp.index.isin(intersect_ix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(fast_to_FoG)+len(check_closely)==len(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fast_to_FoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_to_FoG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = fast_to_FoG.plot(c='g')\n",
    "p.plot(check_closely, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_a = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='A - GENERAL INFORMATION')\n",
    "template_b = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='B - STATE')\n",
    "template_d = pd.read_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\FoG_Subm_for_pandas.xls', sheetname='D - CHANGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assign_dict = {(3373, 'AIALIK'): np.nan,    # no equivalent \n",
    "               (3543, 'ALLEN'): np.nan,     # no equivalent\n",
    "              (92, 'APPLEGATE'): 119,\n",
    "              (102, 'BAKER'): 280,\n",
    "              (105, 'BALTIMORE'): 214,\n",
    "              (165, 'BARNARD'): 184,\n",
    "              (168, 'BARRY'): 971,\n",
    "              (1390, 'BARTLETT'): np.nan,  # no equ\n",
    "              (3372, 'BEAR'): np.nan,      # no equ\n",
    "              (3376, 'BEAR LAKE'): np.nan, # no eq \n",
    "              (97, 'BELOIT'): 302,\n",
    "              (3377, 'BENCH'): np.nan,\n",
    "              (98,'BLACKSTONE'): 980,\n",
    "              (157,'BRILLIANT'): np.nan,\n",
    "              (162,'BRYN MAWR'): 977,\n",
    "              (169,'CASCADE'): 973,\n",
    "              (100,'CATARACT'): 33,\n",
    "              (180,'CHENEGA'): np.nan,\n",
    "              (3379,'CHERNOF'): np.nan,\n",
    "              (152,'CHILDS'): np.nan,\n",
    "              (176,'CLAREMONT NORTH'): 311,\n",
    "              (177,'CLAREMONT WEST'): 312,\n",
    "              (3544,'COLONY'): 203,\n",
    "              (156,'COLUMBIA (627)'): 976,\n",
    "              (178,'CONTACT'): 325,   # is okay as only front variation present in FoG\n",
    "              (167,'COXE'): 978,\n",
    "              (3381,'DESERTED'): np.nan,\n",
    "              (101,'DETACHED'): np.nan,\n",
    "              (3382,'DINGLESTADT'): 364,\n",
    "              (3383,'DOUBLE'): 774,   # ATTENTION: Name is different: DOUBLE in FoG, Big RIver Glacier in LP and Big River Lobe Double Glac in GLIMS\n",
    "              (85,'EKLUTNA'): np.nan,\n",
    "              (3790,'EXCELSIOR'): 981,\n",
    "              (86,'EXIT'): np.nan,\n",
    "              (91,'FALLING'): np.nan,\n",
    "              (3386,'FORK TLIKAKILA'): 777,\n",
    "              (3791,'GREWINGK'): np.nan,\n",
    "              (172,'HARRIMAN'): 975,\n",
    "              (160,'HARVARD'): np.nan,\n",
    "              (3390,'HOLGATE'): 982,\n",
    "              (166,'HOLYOKE'): 185,\n",
    "              (3391,'KACHEMAK'): np.nan,\n",
    "              (3310,'KNIK'): 204,\n",
    "              (4333,'KNIK NORTH'): np.nan,  # part of KNIK (only one polygon in LP)\n",
    "              (4332,'KNIK SOUTH'): np.nan,  # part of KNIK (only one polygon in LP)\n",
    "              (95,'LAWRENCE'): 298,\n",
    "              (173,'LEARNARD'): 100,\n",
    "              (3394,'LITTLE DINGLESTADT'): 360,\n",
    "              (3545,'MARCUS BAKER'): 177,\n",
    "              (96,'MARQUETTE'): 301,\n",
    "              (3546,'MATANUSKA E'): np.nan,  # one polygon together with MATANUSKA W in LP\n",
    "              (3547,'MATANUSKA W'): np.nan,  # one polygon together with MATANUSKA E in LP\n",
    "              (3396,'MC CARTY'): np.nan,\n",
    "              (158,'MEARES'): np.nan,\n",
    "              (3548,'NELCHINA'): np.nan,\n",
    "              (179,'NELLIE JUAN'): np.nan,\n",
    "              (3414,'NORTH FORK TLIKAKILA'): 769,\n",
    "              (3399,'NORTHEASTERN'): 366,    # not absolutely sure if this is the one\n",
    "              (3793,'NORTHWESTERN'): np.nan,\n",
    "              (3794,'NUKA'): np.nan,         # VERY DIFFICULT CASE\n",
    "              (103,'PENNIMAN EAST'): np.nan, # not really clear where the transition is\n",
    "              (104,'PENNIMAN WEST'): 279,    # probably true but not 100% clear \n",
    "              (174,'PORTAGE'): 292,\n",
    "              (3335,'RED'): np.nan,           \n",
    "              (99,'ROARING'): np.nan,\n",
    "              (108,'SADDLEBAG'): np.nan,\n",
    "              (4334,'SCOTT'): np.nan,\n",
    "              (170,'SERPENTINE'): 979,\n",
    "              (3413,'SHAMROCK'): 781,\n",
    "              (151,'SHERIDAN'): np.nan,\n",
    "              (107,'SHERMAN'): np.nan,\n",
    "              (155,'SHOUP'): 970,\n",
    "              (3401,'SKILAK'): np.nan,\n",
    "              (161,'SMITH'): np.nan,\n",
    "              (3549,'SOUTH FORK TSINA'): 179,  # very strange naming, but this is it\n",
    "              (1391,'SPENCER'): np.nan,        # FoG coordinate was wrong\n",
    "              (171,'SURPRISE'): 974,\n",
    "              (3403,'TANAINA'): 792,\n",
    "              (93,'TAYLOR US'): 310,\n",
    "              (3405,'TAZLINA'): np.nan,\n",
    "              (175,'TEBENKOF'): np.nan,\n",
    "              (3550,'TONSINA'): 195,\n",
    "              (1389,'TRAIL'): 307,\n",
    "              (3551,'TSINA'): 178,\n",
    "              (3407,'TURQUOISE'): 787,\n",
    "              (3408,'TUSTUMENA'): np.nan,\n",
    "              (3409,'TUXEDNI'): 928, \n",
    "              (1387,'UNNAMED US0623'): np.nan,    # seems to be part of ID 976 in LP as B table says only 4km long\n",
    "              (106,'UNNAMED US624'): np.nan,      # unclear which is meant; probably no equivalent\n",
    "              (154,'VALDEZ'): 211,\n",
    "              (163,'VASSAR'): 215,\n",
    "              (164,'WELLESLEY'): 972,\n",
    "              (94,'WOLVERINE'): np.nan,\n",
    "              (3580,'WOODWORTH'): np.nan,\n",
    "              (153,'WORTHINGTON'): 181,\n",
    "              (3412,'WORTHMANNS'): np.nan,\n",
    "              (159,'YALE'): np.nan,\n",
    "              (3797,'YALIK'):409}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "political_unit = 'US'\n",
    "geogr_loc = 'Western Alaska Range'\n",
    "year = 2006\n",
    "survey_date = 20069999\n",
    "ref_date = 19509999\n",
    "surv_plat_meth = 'sP'\n",
    "ref_plat_meth = 'aM'\n",
    "remarks_b = 'Outlines derived by manual/automated digitizing based on large-scale 7.5 min topographic quadrangle maps. Some corrected manually.'\n",
    "remarks_d = '1950s: Outlines and elevation derived from topograhic maps, 2006: elevation derived from SPOT5 HRS (SPIRIT)'\n",
    "investigator = 'Raymond LE BRIS and Frank PAUL'\n",
    "spons_agenc = 'Dept. of Geography, University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Sitzerland'\n",
    "ref = 'Le Bris, R. and Paul, F. (2015); Annals of Glaciology, 56(70), pp.184-192.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ids = range(10000, 11000, 1)\n",
    "\n",
    "for ind, row in lp.iterrows():\n",
    "    \n",
    "    if row.ID not in assign_dict.values():\n",
    "        wgms_id = new_ids[0]\n",
    "        new_ids = new_ids[1:]\n",
    "\n",
    "        template_a.loc[ind, 'POLITCAL_UNIT'] = political_unit\n",
    "        template_a.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_a.loc[ind, 'GLACIER_NAME'] = row.glacier_na\n",
    "        template_a.loc[ind, 'GEOGRAPHICAL_LOCATION_GENERAL']  = geogr_loc\n",
    "\n",
    "        repres_point = row.geometry.representative_point()\n",
    "        repres_point_ll = transform_coords(repres_point.x, repres_point.y,32605,4326) \n",
    "\n",
    "        template_a.loc[ind, 'LATITUDE'] = round(repres_point_ll[1], 6)\n",
    "        template_a.loc[ind, 'LONGITUDE'] = round(repres_point_ll[0], 6)\n",
    "        template_a.loc[ind, 'REMARKS'] =  ''\n",
    "\n",
    "\n",
    "        template_b.loc[ind, 'POLITCAL_UNIT'] = political_unit\n",
    "        template_b.loc[ind, 'GLACIER_NAME'] = row.glacier_na\n",
    "        template_b.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_b.loc[ind, 'YEAR'] = year\n",
    "        template_b.loc[ind, 'AREA'] = row.AREA\n",
    "        template_b.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_b.loc[ind, 'SURVEY_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_b.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_b.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_b.loc[ind, 'REFERENCE'] = ref\n",
    "        template_b.loc[ind, 'REMARKS'] = remarks_b\n",
    "\n",
    "        template_d.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        template_d.loc[ind, 'GLACIER_NAME'] = row.glacier_na\n",
    "        template_d.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_d.loc[ind, 'YEAR'] = year\n",
    "        template_d.loc[ind, 'LOWER_BOUND'] = np.nan\n",
    "        template_d.loc[ind, 'UPPER_BOUND'] = np.nan\n",
    "        template_d.loc[ind, 'AREA_SURVEY_YEAR'] = year\n",
    "        template_d.loc[ind, 'THICKNESS_CHANGE'] = row.dh_mean * 1000.  # unit: mm\n",
    "        template_d.loc[ind, 'VOLUME_CHANGE'] = row.dh_mean * row.AREA * 1000.  # unit: m*km2*1000 = 1000m3\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'REFERENCE_DATE_PLATFORM_METHOD'] = ref_plat_meth\n",
    "        template_d.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_d.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_d.loc[ind, 'REFERENCE'] = ref\n",
    "        template_d.loc[ind, 'REMARKS'] = remarks_d\n",
    "    \n",
    "    else:\n",
    "        wgms_id = list(assign_dict.keys())[list(assign_dict.values()).index(row.ID)][0]\n",
    "        gname = list(assign_dict.keys())[list(assign_dict.values()).index(row.ID)][1]\n",
    "        \n",
    "        template_b.loc[ind, 'POLITCAL_UNIT'] = political_unit\n",
    "        template_b.loc[ind, 'GLACIER_NAME'] = gname\n",
    "        template_b.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_b.loc[ind, 'YEAR'] = year\n",
    "        template_b.loc[ind, 'AREA'] = row.AREA\n",
    "        template_b.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_b.loc[ind, 'SURVEY_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_b.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_b.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_b.loc[ind, 'REFERENCE'] = ref\n",
    "        template_b.loc[ind, 'REMARKS'] = remarks_b\n",
    "\n",
    "        template_d.loc[ind, 'POLITICAL_UNIT'] = political_unit\n",
    "        template_d.loc[ind, 'GLACIER_NAME'] = row.glacier_na\n",
    "        template_d.loc[ind, 'WGMS_ID'] = wgms_id\n",
    "        template_d.loc[ind, 'YEAR'] = year\n",
    "        template_d.loc[ind, 'LOWER_BOUND'] = np.nan\n",
    "        template_d.loc[ind, 'UPPER_BOUND'] = np.nan\n",
    "        template_d.loc[ind, 'AREA_SURVEY_YEAR'] = year\n",
    "        template_d.loc[ind, 'THICKNESS_CHANGE'] = row.dh_mean * 1000.  # unit: mm\n",
    "        template_d.loc[ind, 'VOLUME_CHANGE'] = row.dh_mean * row.AREA * 1000.  # unit: m*km2*1000 = 1000m3\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE'] = survey_date\n",
    "        template_d.loc[ind, 'SURVEY_DATE_PLATFORM_METHOD'] = surv_plat_meth\n",
    "        template_d.loc[ind, 'REFERENCE_DATE'] = ref_date\n",
    "        template_d.loc[ind, 'REFERENCE_DATE_PLATFORM_METHOD'] = ref_plat_meth\n",
    "        template_d.loc[ind, 'INVESTIGATOR'] = investigator\n",
    "        template_d.loc[ind, 'SPONSORING_AGENCY'] = spons_agenc\n",
    "        template_d.loc[ind, 'REFERENCE'] = ref\n",
    "        template_d.loc[ind, 'REMARKS'] = remarks_d\n",
    "                   \n",
    "template_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_a.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_A.xls', index=False)\n",
    "template_b.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_B.xls', index=False)\n",
    "template_d.to_excel('C:\\\\users\\\\jlandman\\\\Desktop\\\\RL_FP_to_FoG_automatic_D.xls', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(template_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(template_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(template_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(~np.isnan(list(assign_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign GLIMS IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glims = salem.utils.read_shapefile('C:\\\\Users\\\\jlandman\\\\Desktop\\\\glims_db_20160429\\\\glims_polygons_alaska.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glims_id_success = {}\n",
    "glims_id_fail = {}\n",
    "\n",
    "ct=0\n",
    "for k, row in template_a.iterrows():\n",
    "    gp = shpg.Point(row.LONGITUDE, row.LATITUDE)\n",
    "    rectangle = shpg.Polygon([(row.LONGITUDE-0.1, row.LATITUDE-0.1), (row.LONGITUDE-0.1, row.LATITUDE+0.1), (row.LONGITUDE+0.1, row.LATITUDE-0.1), (row.LONGITUDE+0.1, row.LATITUDE-0.1)])\n",
    "    subset = glims[glims.intersects(rectangle)]\n",
    "    #if isinstance(subset, pd.DataFrame):\n",
    "    for i, r in subset.iterrows():\n",
    "        ct+=1\n",
    "        if r.geometry.contains(gp):\n",
    "            glims_id_success[row.WGMS_ID] = r['glac_id']\n",
    "        else:\n",
    "            glims_id_fail[row.WGMS_ID] = np.nan\n",
    "        print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(glims_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(template_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
